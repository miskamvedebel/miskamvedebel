{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'patents.json'\n",
    "patents_data = pd.read_json(FILE)\n",
    "title = [p['patent_title']for p in patents_data['patents']]\n",
    "abstract =  [p['patent_abstract'] for p in patents_data['patents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_none_removed = []\n",
    "abstract_none_removed = []\n",
    "removed_titles = []\n",
    "for t, a in zip(title, abstract):\n",
    "    try:\n",
    "        if a != None:\n",
    "            abstract_none_removed.append(a)\n",
    "            titles_none_removed.append(t)\n",
    "    except:\n",
    "        removed_titles.append((t, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Clarification of black ammonium polyphosphate liquids--recycling of byproduct \"\"tops\"\"\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_none_removed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process allows essentially all of the nitrogen and P.sub.2 O.sub.5 values in the treated black ammonium polyphosphate liquid to be recovered in the form of valuable clarified product. In the process, a heel of black liquid fertilizer is first clarified by a prior-art procedure using flocculants. The improvement over the prior art picks up with the byproduct tops which is then diluted with the water of formulation required to dissolve additional amounts of ammonium polyphosphate melt. This diluted liquid is filtered, the filter cake containing upwards of 99 percent of the undesirable black carbonaceous material is discarded, and the clear filtrate, which contains essentially all of the nitrogen and P.sub.2 O.sub.5 originally in the tops, is then used in lieu of the prior-art water of formulation to dissolve additional ammonium polyphosphate melt to produce more black ammonium polyphosphate liquid.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_none_removed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer = Tokenizer(filters=filters, lower=True, split=' ', num_words=None, char_level=False)\n",
    "#fitting tokenizer and transforming that to tokens\n",
    "tokenizer.fit_on_texts(abstract_none_removed)\n",
    "sequences = tokenizer.texts_to_sequences(abstract_none_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 18, 548, 765, 309, 3, 1, 393, 4, 1032]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the process allows essentially all of the nitrogen and p'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.index_word[ind] for ind in sequences[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process allows essentially all of the nitrogen and P.sub.2 O.sub.5 values in the treated black a'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_none_removed[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_punct = '!\"#$%&()*+-/<=>?@[\\\\]^_`{|}~\\t\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_punct = Tokenizer(filters=filters_punct, lower=False, split=' ', num_words=None, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_part = abstract_none_removed[:4000]\n",
    "title_part = titles_none_removed[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting tokenizer and transforming that to tokens - different filter\n",
    "tokenizer_punct.fit_on_texts(abstract_part)\n",
    "sequences = tokenizer_punct.texts_to_sequences(abstract_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 35, 420, 838, 332, 3, 1, 688, 4, 10492]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process allows essentially all of the nitrogen and P.sub.2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer_punct.index_word[ind] for ind in sequences[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "training_size = 50\n",
    "#iterating thru the sequences \n",
    "for seq in sequences:\n",
    "    for i in range(training_size, len(seq)):\n",
    "        #in case 50 then taking seq[0:51]\n",
    "        extract = seq[i-training_size: i+1]\n",
    "        #features --> seq[0:50]\n",
    "        features.append(extract[:-1])\n",
    "        #labels --> seq[51]\n",
    "        labels.append(extract[-1])\n",
    "#transforming to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    9,    35,   420,   838,   332,     3,     1,   688,     4,\n",
       "       10492, 10493,   973,     8,     1,   377,   888,  1561,  7414,\n",
       "          31,     5,    20,   232,     8,     1,    72,     3,  1801,\n",
       "        2701,   800,    68,     1,   455,     2, 14184,     3,   888,\n",
       "          31,  4002,     6,    26,  2701,    15,     2,   406,  2498,\n",
       "        2343,    97, 14185,     9,   801])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process allows essentially all of the nitrogen and P.sub.2 O.sub.5 values in the treated black ammonium polyphosphate liquid to be recovered in the form of valuable clarified product. In the process, a heel of black liquid fertilizer is first clarified by a prior art procedure using flocculants. The improvement'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer_punct.index_word[ind] for ind in features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 'over')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0], tokenizer_punct.index_word[labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324747, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building one_hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22896\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer_punct.index_word) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating labels vector\n",
    "labels_vector = np.zeros(shape=(len(features), num_words), dtype=np.int8)\n",
    "\n",
    "for ex_index, word_index in enumerate(labels):\n",
    "    labels_vector[ex_index, word_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_punct.index_word[np.argmax(labels_vector[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILE = 'glove.6B.100d.txt'\n",
    "glove = np.loadtxt(EMBEDDINGS_FILE, dtype=str, comments=None)\n",
    "vectors = glove[:, 1:].astype(float)\n",
    "words = glove[:, 0]\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lookup['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros(shape=(num_words, vectors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokenizer_punct.index_word:\n",
    "    vector = word_lookup.get(tokenizer_punct.index_word.get(i), None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i, :] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_punct.index_word.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lookup['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/mlebedev/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#identifying model:\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=num_words, \n",
    "                    input_length=training_size, \n",
    "                    weights=[embedding_matrix],\n",
    "                    output_dim=100,\n",
    "                    trainable=False,\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(Masking(mask_value=0.0))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           2289600   \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 22896)             1488240   \n",
      "=================================================================\n",
      "Total params: 3,824,240\n",
      "Trainable params: 1,534,640\n",
      "Non-trainable params: 2,289,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint('./models/model.h5', save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((324747, 50), (324747, 22896))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, labels_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = features[:100000], labels_vector[:100000]\n",
    "X_valid, y_valid = features[100000:110000], labels_vector[100000:110000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/150\n",
      "100000/100000 [==============================] - 85s 854us/step - loss: 8.3366 - acc: 0.0662 - val_loss: 7.0410 - val_acc: 0.0935\n",
      "Epoch 2/150\n",
      "100000/100000 [==============================] - 84s 844us/step - loss: 6.9174 - acc: 0.0898 - val_loss: 6.9852 - val_acc: 0.0935\n",
      "Epoch 3/150\n",
      "100000/100000 [==============================] - 92s 916us/step - loss: 6.8533 - acc: 0.0898 - val_loss: 6.9894 - val_acc: 0.0935\n",
      "Epoch 4/150\n",
      "100000/100000 [==============================] - 89s 887us/step - loss: 6.8195 - acc: 0.0898 - val_loss: 6.9949 - val_acc: 0.0935\n",
      "Epoch 5/150\n",
      "100000/100000 [==============================] - 91s 914us/step - loss: 6.7944 - acc: 0.0898 - val_loss: 6.9974 - val_acc: 0.0935\n",
      "Epoch 6/150\n",
      "100000/100000 [==============================] - 92s 922us/step - loss: 6.7696 - acc: 0.0898 - val_loss: 6.9909 - val_acc: 0.0935\n",
      "Epoch 7/150\n",
      "100000/100000 [==============================] - 93s 935us/step - loss: 6.7380 - acc: 0.0908 - val_loss: 6.9631 - val_acc: 0.0936\n",
      "Epoch 8/150\n",
      "100000/100000 [==============================] - 100s 996us/step - loss: 6.7010 - acc: 0.0936 - val_loss: 6.9327 - val_acc: 0.1007\n",
      "Epoch 9/150\n",
      "100000/100000 [==============================] - 99s 986us/step - loss: 6.6629 - acc: 0.0969 - val_loss: 6.9152 - val_acc: 0.1037\n",
      "Epoch 10/150\n",
      "100000/100000 [==============================] - 93s 925us/step - loss: 6.6388 - acc: 0.0981 - val_loss: 6.9034 - val_acc: 0.1086\n",
      "Epoch 11/150\n",
      "100000/100000 [==============================] - 96s 955us/step - loss: 6.6120 - acc: 0.1008 - val_loss: 6.8916 - val_acc: 0.1088\n",
      "Epoch 12/150\n",
      "100000/100000 [==============================] - 98s 980us/step - loss: 6.5868 - acc: 0.1025 - val_loss: 6.8779 - val_acc: 0.1147\n",
      "Epoch 13/150\n",
      "100000/100000 [==============================] - 87s 869us/step - loss: 6.5578 - acc: 0.1039 - val_loss: 6.8712 - val_acc: 0.1163\n",
      "Epoch 14/150\n",
      "100000/100000 [==============================] - 88s 885us/step - loss: 6.5312 - acc: 0.1062 - val_loss: 6.8625 - val_acc: 0.1173\n",
      "Epoch 15/150\n",
      "100000/100000 [==============================] - 121s 1ms/step - loss: 6.5034 - acc: 0.1075 - val_loss: 6.8448 - val_acc: 0.1177\n",
      "Epoch 16/150\n",
      "100000/100000 [==============================] - 99s 988us/step - loss: 6.4713 - acc: 0.1099 - val_loss: 6.8209 - val_acc: 0.1223\n",
      "Epoch 17/150\n",
      "100000/100000 [==============================] - 97s 966us/step - loss: 6.4353 - acc: 0.1123 - val_loss: 6.8001 - val_acc: 0.1275\n",
      "Epoch 18/150\n",
      "100000/100000 [==============================] - 93s 925us/step - loss: 6.4035 - acc: 0.1146 - val_loss: 6.7831 - val_acc: 0.1287\n",
      "Epoch 19/150\n",
      "100000/100000 [==============================] - 96s 962us/step - loss: 6.3701 - acc: 0.1165 - val_loss: 6.7648 - val_acc: 0.1307\n",
      "Epoch 20/150\n",
      "100000/100000 [==============================] - 91s 906us/step - loss: 6.3403 - acc: 0.1188 - val_loss: 6.7470 - val_acc: 0.1315\n",
      "Epoch 21/150\n",
      "100000/100000 [==============================] - 98s 979us/step - loss: 6.3084 - acc: 0.1206 - val_loss: 6.7373 - val_acc: 0.1332\n",
      "Epoch 22/150\n",
      "100000/100000 [==============================] - 96s 959us/step - loss: 6.2815 - acc: 0.1227 - val_loss: 6.7235 - val_acc: 0.1340\n",
      "Epoch 23/150\n",
      "100000/100000 [==============================] - 99s 989us/step - loss: 6.2564 - acc: 0.1242 - val_loss: 6.7150 - val_acc: 0.1349\n",
      "Epoch 24/150\n",
      "100000/100000 [==============================] - 100s 997us/step - loss: 6.2314 - acc: 0.1250 - val_loss: 6.7042 - val_acc: 0.1352\n",
      "Epoch 25/150\n",
      "100000/100000 [==============================] - 86s 865us/step - loss: 6.2088 - acc: 0.1253 - val_loss: 6.7036 - val_acc: 0.1353\n",
      "Epoch 26/150\n",
      "100000/100000 [==============================] - 85s 854us/step - loss: 6.1833 - acc: 0.1261 - val_loss: 6.6908 - val_acc: 0.1358\n",
      "Epoch 27/150\n",
      "100000/100000 [==============================] - 86s 856us/step - loss: 6.1630 - acc: 0.1273 - val_loss: 6.6865 - val_acc: 0.1373\n",
      "Epoch 28/150\n",
      "100000/100000 [==============================] - 85s 855us/step - loss: 6.1383 - acc: 0.1288 - val_loss: 6.6831 - val_acc: 0.1373\n",
      "Epoch 29/150\n",
      "100000/100000 [==============================] - 85s 854us/step - loss: 6.1176 - acc: 0.1291 - val_loss: 6.6736 - val_acc: 0.1384\n",
      "Epoch 30/150\n",
      "100000/100000 [==============================] - 86s 855us/step - loss: 6.0977 - acc: 0.1297 - val_loss: 6.6681 - val_acc: 0.1403\n",
      "Epoch 31/150\n",
      "100000/100000 [==============================] - 85s 853us/step - loss: 6.0715 - acc: 0.1308 - val_loss: 6.6611 - val_acc: 0.1408\n",
      "Epoch 32/150\n",
      "100000/100000 [==============================] - 93s 932us/step - loss: 6.0510 - acc: 0.1315 - val_loss: 6.6551 - val_acc: 0.1406\n",
      "Epoch 33/150\n",
      "100000/100000 [==============================] - 88s 878us/step - loss: 6.0318 - acc: 0.1321 - val_loss: 6.6468 - val_acc: 0.1413\n",
      "Epoch 34/150\n",
      "100000/100000 [==============================] - 117s 1ms/step - loss: 6.0078 - acc: 0.1316 - val_loss: 6.6418 - val_acc: 0.1405\n",
      "Epoch 35/150\n",
      "100000/100000 [==============================] - 115s 1ms/step - loss: 5.9909 - acc: 0.1333 - val_loss: 6.6346 - val_acc: 0.1413\n",
      "Epoch 36/150\n",
      "100000/100000 [==============================] - 119s 1ms/step - loss: 5.9664 - acc: 0.1332 - val_loss: 6.6310 - val_acc: 0.1403\n",
      "Epoch 37/150\n",
      "100000/100000 [==============================] - 113s 1ms/step - loss: 5.9493 - acc: 0.1347 - val_loss: 6.6232 - val_acc: 0.1416\n",
      "Epoch 38/150\n",
      "100000/100000 [==============================] - 104s 1ms/step - loss: 5.9274 - acc: 0.1344 - val_loss: 6.6224 - val_acc: 0.1420\n",
      "Epoch 39/150\n",
      "100000/100000 [==============================] - 121s 1ms/step - loss: 5.9092 - acc: 0.1346 - val_loss: 6.6167 - val_acc: 0.1426\n",
      "Epoch 40/150\n",
      "100000/100000 [==============================] - 102s 1ms/step - loss: 5.8858 - acc: 0.1346 - val_loss: 6.6158 - val_acc: 0.1414\n",
      "Epoch 41/150\n",
      "100000/100000 [==============================] - 102s 1ms/step - loss: 5.8684 - acc: 0.1350 - val_loss: 6.6082 - val_acc: 0.1426\n",
      "Epoch 42/150\n",
      "100000/100000 [==============================] - 89s 888us/step - loss: 5.8504 - acc: 0.1346 - val_loss: 6.6024 - val_acc: 0.1418\n",
      "Epoch 43/150\n",
      "100000/100000 [==============================] - 88s 879us/step - loss: 5.8260 - acc: 0.1360 - val_loss: 6.6008 - val_acc: 0.1423\n",
      "Epoch 44/150\n",
      "100000/100000 [==============================] - 88s 878us/step - loss: 5.8102 - acc: 0.1363 - val_loss: 6.5971 - val_acc: 0.1410\n",
      "Epoch 45/150\n",
      "100000/100000 [==============================] - 88s 877us/step - loss: 5.7901 - acc: 0.1363 - val_loss: 6.5949 - val_acc: 0.1421\n",
      "Epoch 46/150\n",
      "100000/100000 [==============================] - 88s 875us/step - loss: 5.7671 - acc: 0.1369 - val_loss: 6.5879 - val_acc: 0.1423\n",
      "Epoch 47/150\n",
      "100000/100000 [==============================] - 88s 877us/step - loss: 5.7520 - acc: 0.1378 - val_loss: 6.5838 - val_acc: 0.1433\n",
      "Epoch 48/150\n",
      "100000/100000 [==============================] - 88s 877us/step - loss: 5.7322 - acc: 0.1368 - val_loss: 6.5796 - val_acc: 0.1431\n",
      "Epoch 49/150\n",
      "100000/100000 [==============================] - 88s 877us/step - loss: 5.7119 - acc: 0.1371 - val_loss: 6.5784 - val_acc: 0.1426\n",
      "Epoch 50/150\n",
      "100000/100000 [==============================] - 88s 881us/step - loss: 5.6994 - acc: 0.1363 - val_loss: 6.5740 - val_acc: 0.1444\n",
      "Epoch 51/150\n",
      "100000/100000 [==============================] - 105s 1ms/step - loss: 5.6751 - acc: 0.1386 - val_loss: 6.5683 - val_acc: 0.1436\n",
      "Epoch 52/150\n",
      "100000/100000 [==============================] - 97s 965us/step - loss: 5.6548 - acc: 0.1390 - val_loss: 6.5672 - val_acc: 0.1444\n",
      "Epoch 53/150\n",
      "100000/100000 [==============================] - 90s 901us/step - loss: 5.6414 - acc: 0.1389 - val_loss: 6.5654 - val_acc: 0.1442\n",
      "Epoch 54/150\n",
      "100000/100000 [==============================] - 89s 889us/step - loss: 5.6262 - acc: 0.1385 - val_loss: 6.5621 - val_acc: 0.1449\n",
      "Epoch 55/150\n",
      "100000/100000 [==============================] - 87s 873us/step - loss: 5.6050 - acc: 0.1383 - val_loss: 6.5616 - val_acc: 0.1451\n",
      "Epoch 56/150\n",
      "100000/100000 [==============================] - 89s 891us/step - loss: 5.5889 - acc: 0.1385 - val_loss: 6.5580 - val_acc: 0.1438\n",
      "Epoch 57/150\n",
      "100000/100000 [==============================] - 88s 882us/step - loss: 5.5813 - acc: 0.1390 - val_loss: 6.5570 - val_acc: 0.1447\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 88s 879us/step - loss: 5.5641 - acc: 0.1399 - val_loss: 6.5585 - val_acc: 0.1449\n",
      "Epoch 59/150\n",
      "100000/100000 [==============================] - 88s 878us/step - loss: 5.5407 - acc: 0.1393 - val_loss: 6.5568 - val_acc: 0.1455\n",
      "Epoch 60/150\n",
      "100000/100000 [==============================] - 89s 888us/step - loss: 5.5292 - acc: 0.1391 - val_loss: 6.5538 - val_acc: 0.1452\n",
      "Epoch 61/150\n",
      "100000/100000 [==============================] - 92s 916us/step - loss: 5.5132 - acc: 0.1392 - val_loss: 6.5599 - val_acc: 0.1445\n",
      "Epoch 62/150\n",
      "100000/100000 [==============================] - 98s 979us/step - loss: 5.5011 - acc: 0.1408 - val_loss: 6.5587 - val_acc: 0.1436\n",
      "Epoch 63/150\n",
      "100000/100000 [==============================] - 90s 902us/step - loss: 5.4882 - acc: 0.1403 - val_loss: 6.5557 - val_acc: 0.1454\n",
      "Epoch 64/150\n",
      "100000/100000 [==============================] - 91s 906us/step - loss: 5.4731 - acc: 0.1411 - val_loss: 6.5566 - val_acc: 0.1460\n",
      "Epoch 65/150\n",
      "100000/100000 [==============================] - 89s 890us/step - loss: 5.4576 - acc: 0.1417 - val_loss: 6.5561 - val_acc: 0.1456\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,  y_train, \n",
    "                    batch_size=2048, epochs=150,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 22896)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 470us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.553760202026367, 0.1452]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['The process allows essentially all of the nitrogen and P.sub.2 O.sub.5 values in the treated black ammonium polyphosphate liquid to be recovered in the form of valuable clarified product. In the process, a heel of black liquid fertilizer is first clarified by a prior-art procedure using flocculants. The improvement over the prior art picks up with the byproduct tops which is then diluted with the water of formulation required to dissolve additional amounts of ammonium polyphosphate melt. This diluted liquid is filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = np.array(tokenizer_punct.texts_to_sequences(X)[0][30:80]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the process, a heel of black liquid fertilizer is first clarified by a prior art procedure using flocculants. The improvement over the prior art picks up with the byproduct tops which is then diluted with the water of formulation required to dissolve additional amounts of ammonium polyphosphate melt. This diluted'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer_punct.index_word[ind] for ind in transformed[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_punct.index_word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    9,    35,   420,   838,   332,     3,     1,   688,     4,\n",
       "       10492, 10493,   973,     8,     1,   377,   888,  1561,  7414,\n",
       "          31,     5,    20,   232,     8,     1,    72,     3,  1801,\n",
       "        2701,   800,    68,     1,   455,     2, 14184,     3,   888,\n",
       "          31,  4002,     6,    26,  2701,    15,     2,   406,  2498,\n",
       "        2343,    97, 14185,     9,   801,     3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(transformed,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    9,    35,   420,   838,   332,     3,     1,   688,     4,\n",
       "       10492, 10493,   973,     8,     1,   377,   888,  1561,  7414,\n",
       "          31,     5,    20,   232,     8,     1,    72,     3,  1801,\n",
       "        2701,   800,    68,     1,   455,     2, 14184,     3,   888,\n",
       "          31,  4002,     6,    26,  2701,    15,     2,   406,  2498,\n",
       "        2343,    97, 14185,     9,   801,     3,     1,     9,     9,\n",
       "           9,     9,     9,     9,     9,     9])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the process, a heel of black liquid fertilizer is first clarified by a prior art procedure using flocculants. The improvement over the prior art picks up with the byproduct tops which is then diluted with the water of formulation required to dissolve additional amounts of ammonium polyphosphate melt. This diluted and a second and a second portion of the first\n"
     ]
    }
   ],
   "source": [
    "generated_indexes = transformed\n",
    "size = transformed.shape[1]\n",
    "for i in range(10):\n",
    "    last_index = size + i\n",
    "    to_pass = generated_indexes[i:last_index].reshape(1, -1)\n",
    "    generated_indexes = np.append(generated_indexes, model.predict_classes(to_pass))\n",
    "print(' '.join(tokenizer_punct.index_word[ind] for ind in generated_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,   455,     2, 14184,     3,   888,    31,  4002,     6,\n",
       "          26,  2701,    15,     2,   406,  2498,  2343,    97, 14185,\n",
       "           9,   801,   254,     1,   406,  2498,  7415,   248,    13,\n",
       "           1,  2206,  6577,    17,     6,    91,  2499,    13,     1,\n",
       "          27,     3,  7416,   555,     5,  3362,   342,   879,     3,\n",
       "        1561,  7414,  5886,   166,  2499,     4,     2,    37,     4,\n",
       "           2,    37,    45,     3,     1,    26])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(np.append(transformed, [3, 1, 9])[3:53].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
