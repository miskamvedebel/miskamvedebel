{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jxxU1RWYMmNVgxQ0_3UQ2rQK5opnXa_X",
      "authorship_tag": "ABX9TyOxqLjnyF5daqsrwSK14Cic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miskamvedebel/miskamvedebel/blob/master/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U17VKWJ7hZYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "652fd1ea-1764-41c0-8493-1570cb9124b2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kVEIylHhiqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_excel('/content/drive/My Drive/Colab Notebooks/data/descriptions.xlsx')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WstlAAFhdJM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7crjPTQk515",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exc = data.full_description.tolist()\n",
        "pattern = '[0-9]'\n",
        "exc = [re.sub(pattern, '', txt) for txt in exc]\n",
        "exc = [wordpunct_tokenize(sent) for sent in exc]\n",
        "exc_no_stop = []\n",
        "for sent in exc:\n",
        "  exc_no_stop.append(' '.join(lemmatizer.lemmatize(word, pos='v') for word in sent if word not in stop_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quMEZ-0NnqaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = '!\"#$%&()*+,-./:;’‘\\'–<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "tokenizer = Tokenizer(filters=filters, char_level=False, num_words=None, lower=True, split=' ')\n",
        "tokenizer.fit_on_texts(exc_no_stop)\n",
        "sequences = tokenizer.texts_to_sequences(exc_no_stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYtEOLe3lLlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setting up number of topics\n",
        "n_topics = 6\n",
        "# dirichlet distribution parameter\n",
        "alpha = 1\n",
        "# hyperparameter\n",
        "eta = 0.001\n",
        "# gibbs sampling\n",
        "iterations = 4\n",
        "#length of vocab\n",
        "N = len(tokenizer.index_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiiycBU1mXWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "LDA assumes the following generative process for each document w in a corpus D:\n",
        "\n",
        "Choose N ∼ Poisson(ξ).\n",
        "Choose θ ∼ Dir(α).\n",
        "For each of the N words wn:\n",
        "(a) Choose a topic zn ∼ Multinomial(θ).\n",
        "(b) Choose a word wn from p(wn | zn ,β), a multinomial probability conditioned on the topic zn.\n",
        "'''\n",
        "# Randomly assign topics to each word\n",
        "# Generate word-topic count\n",
        "\n",
        "word_topic = np.zeros(shape=(N, n_topics))\n",
        "topic_assignment = [np.zeros(shape=(len(d))) for d in sequences]\n",
        "document_topic = np.zeros(shape=(len(sequences), n_topics))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncsa7pH6s7Zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics = set(np.arange(n_topics))\n",
        "\n",
        "for d, document in enumerate(sequences):\n",
        "  for t, token in enumerate(document):\n",
        "    topic = np.random.choice(n_topics, 1)\n",
        "    topic_assignment[d][t] = topic\n",
        "    word_topic[token-1][topic] += 1\n",
        "  \n",
        "  unique, counts = np.unique(topic_assignment[d], return_counts=True)\n",
        "  if unique.shape[0] != n_topics:\n",
        "        missing = list(topics.difference(unique))\n",
        "        for ms in missing:\n",
        "          counts = np.insert(counts, ms, 0)\n",
        "\n",
        "  document_topic[d, :] = counts\n",
        "\n",
        "for it in range(iterations):\n",
        "  for di, document in enumerate(sequences):\n",
        "    for ti, token in enumerate(document):\n",
        "      \n",
        "      t0 = topic_assignment[di][ti]\n",
        "\n",
        "      denom_a = np.sum(document_topic[di, :]) + n_topics * alpha\n",
        "      denom_b = np.sum(word_topic, axis=0) + N * eta\n",
        "\n",
        "      p_z = (word_topic[token-1, :] + eta) / denom_b * (document_topic[di, :] + alpha) / denom_a\n",
        "      t1 = np.random.choice(n_topics, 1, p=p_z/np.sum(p_z))\n",
        "\n",
        "      topic_assignment[di][ti] = t1\n",
        "      word_topic[token-1][t1] += 1\n",
        "\n",
        "      unique, counts = np.unique(topic_assignment[di], return_counts=True)\n",
        "      if unique.shape[0] != n_topics:\n",
        "        missing = list(topics.difference(unique))\n",
        "        for ms in missing:\n",
        "          counts = np.insert(counts, ms, 0) \n",
        "      \n",
        "      document_topic[di, :] = counts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guPxPPyD0Jx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resuls = pd.DataFrame(word_topic, index=tokenizer.index_word.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3iUGpz2Rzyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6329e09a-33f6-464d-8c4e-884f342d5c68"
      },
      "source": [
        "pd.set_option('display.max_rows', 1000)\n",
        "resuls[0].sort_values(ascending=False)[:10]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nbsp    2800.0\n",
              "you     2742.0\n",
              "day     1918.0\n",
              "time    1697.0\n",
              "the     1674.0\n",
              "take    1590.0\n",
              "one     1545.0\n",
              "tour    1479.0\n",
              "get     1362.0\n",
              "and     1348.0\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wUAK_0QEp_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b0b3bf9c-725c-4ce5-bd04-977373af34b2"
      },
      "source": [
        "resuls[1].sort_values(ascending=False)[:10]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "you      2588.0\n",
              "nbsp     2332.0\n",
              "take     1735.0\n",
              "tour     1714.0\n",
              "day      1673.0\n",
              "one      1609.0\n",
              "time     1603.0\n",
              "visit    1520.0\n",
              "get      1452.0\n",
              "the      1395.0\n",
              "Name: 1, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61UnebwzEvml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b0933fce-eb90-4c33-af58-348d910c22d7"
      },
      "source": [
        "resuls[2].sort_values(ascending=False)[:10]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "you      2797.0\n",
              "nbsp     2107.0\n",
              "day      2085.0\n",
              "time     1801.0\n",
              "the      1576.0\n",
              "take     1517.0\n",
              "one      1495.0\n",
              "water    1481.0\n",
              "lunch    1385.0\n",
              "tour     1367.0\n",
              "Name: 2, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8aleI3VE0xc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eda88b9d-fbfe-4ea4-c8e4-88c0a1c90f88"
      },
      "source": [
        "resuls[3].sort_values(ascending=False)[:10]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "you      2838.0\n",
              "nbsp     2305.0\n",
              "take     1832.0\n",
              "day      1678.0\n",
              "time     1617.0\n",
              "the      1563.0\n",
              "tour     1520.0\n",
              "one      1461.0\n",
              "get      1379.0\n",
              "water    1318.0\n",
              "Name: 3, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unb7ERJhBhD7",
        "colab_type": "text"
      },
      "source": [
        "**Gensim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1xzatE5BmyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usSUjoNf8d9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exc = data.full_description.tolist()\n",
        "exc = [wordpunct_tokenize(sent) for sent in exc]\n",
        "exc_no_stop = []\n",
        "for sent in exc:\n",
        "  temp = [lemmatizer.lemmatize(word) for word in sent if word not in stop_words]\n",
        "  exc_no_stop.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF1mvlmPiQWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = []\n",
        "for seq in sequences:\n",
        "  text.append([tokenizer.index_word[tok] for tok in seq])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilfo_lqcUPJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = Dictionary(text)\n",
        "corpus = [dictionary.doc2bow(txt) for txt in text]\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R571Hz33d8VD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7f6fa8fe-6868-43c9-c773-078bbca0c162"
      },
      "source": [
        "lda.print_topics()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.022*\"nbsp\" + 0.008*\"you\" + 0.007*\"park\" + 0.007*\"take\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"one\" + 0.006*\"and\" + 0.005*\"water\" + 0.005*\"the\"'),\n",
              " (1,\n",
              "  '0.010*\"you\" + 0.009*\"time\" + 0.007*\"the\" + 0.007*\"one\" + 0.006*\"get\" + 0.006*\"explore\" + 0.006*\"take\" + 0.006*\"day\" + 0.006*\"visit\" + 0.006*\"nbsp\"'),\n",
              " (2,\n",
              "  '0.011*\"you\" + 0.011*\"beach\" + 0.009*\"day\" + 0.008*\"water\" + 0.008*\"swim\" + 0.007*\"lunch\" + 0.007*\"take\" + 0.006*\"back\" + 0.006*\"nbsp\" + 0.006*\"cruise\"'),\n",
              " (3,\n",
              "  '0.014*\"you\" + 0.007*\"tour\" + 0.007*\"day\" + 0.007*\"nbsp\" + 0.006*\"take\" + 0.006*\"the\" + 0.006*\"see\" + 0.006*\"get\" + 0.005*\"island\" + 0.005*\"one\"'),\n",
              " (4,\n",
              "  '0.010*\"tour\" + 0.009*\"nbsp\" + 0.007*\"one\" + 0.007*\"dive\" + 0.007*\"you\" + 0.007*\"local\" + 0.007*\"visit\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"take\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew3KRNlq-oZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}